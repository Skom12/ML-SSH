{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed8a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/myenv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/jovyan/my-conda-envs/myenv/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf679be-78d8-4612-b4c4-3929c6fe8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b8f27",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b18bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir):                                                                                                  \n",
    "    r = []                                                                                                            \n",
    "    subdirs = [x[0] for x in os.walk(dir)]\n",
    "    for subdir in subdirs:\n",
    "        files = os.walk(subdir).__next__()[2]\n",
    "        if (len(files) > 0):                                                                                          \n",
    "            for file in files:\n",
    "                r.append(os.path.join(subdir, file))                   \n",
    "    return r\n",
    "\n",
    "def custom_split(x):\n",
    "    return [x[i:i+20].strip() for i in range(0, len(x), 20)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f92cd",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31410a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='./CSV_Data/Training'\n",
    "test_path='./CSV_Data/Performance Test'\n",
    "validation_path='./CSV_Data/Validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1d8be",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc45dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self,dir_path,start_index,end_index):\n",
    "        self.x=None\n",
    "        self.y = None\n",
    "        self.vocab= dict()\n",
    "        \n",
    "        self.load(dir_path,start_index,end_index)\n",
    "        self.tosplit()\n",
    "        self.build_vocab()\n",
    "        self.tokenize()\n",
    "        print(\"#### Done ####\")\n",
    "        \n",
    "    def len_vocab(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def load(self,dir_path,start_index,end_index):\n",
    "        print('#### Loading files ####')\n",
    "        files=list_files(dir_path)\n",
    "        files.sort()\n",
    "        files=files[start_index:end_index]\n",
    "        dataframes=[]\n",
    "        req_cols=[\"hex_values\",\"class\"]\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            df = pd.read_csv(file,sep='\\t',usecols=req_cols)\n",
    "            dataframes.append(df)\n",
    "        data=pd.concat(dataframes,ignore_index=True)\n",
    "        self.y=data['class']\n",
    "        self.x=data['hex_values']\n",
    "    \n",
    "    def tosplit(self):\n",
    "        for idx, value in self.x.iteritems():\n",
    "            self.x[idx]=custom_split(value)\n",
    "          \n",
    "    def build_vocab(self):  \n",
    "        print('#### Building vocab ####')     \n",
    "        i=1\n",
    "        for idx, value in self.x.iteritems():\n",
    "            for element in value:\n",
    "                if element in self.vocab:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.vocab[element]=i\n",
    "                    i=i+1\n",
    "            \n",
    "    def tokenize(self):\n",
    "        print('#### Tokenization ####')\n",
    "        for idx, value in self.x.iteritems():\n",
    "            for i in range(len(value)):\n",
    "                try:\n",
    "                    value[i]=self.vocab[value[i]]\n",
    "                except:\n",
    "                    value[i]=0\n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63d455",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6dacb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_set(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.to_tensor()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    def __setitem__(self,idx,value):\n",
    "        self.x[idx]=value[0]\n",
    "        self.y[idx]=value[1]\n",
    "    def to_tensor(self):\n",
    "        for i in range(len(self.x)):\n",
    "            temp=list(self[i])\n",
    "            temp[0]=torch.Tensor(temp[0]).int()\n",
    "            temp[1]=float(temp[1])\n",
    "            temp=tuple(temp)\n",
    "            self[i]=temp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "876b67ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading files ####\n",
      "./CSV_Data/Training/basic-V_7_8_P1-16.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-24.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-32.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-64.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-16.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-24.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-32.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-64.csv\n",
      "#### Building vocab ####\n",
      "#### Tokenization ####\n",
      "#### Done ####\n",
      "CPU times: user 33.2 s, sys: 1.7 s, total: 34.9 s\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data=preprocess(train_path,21,29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d945dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading files ####\n",
      "./CSV_Data/Performance Test/V_7_8_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-32.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-32.csv\n",
      "./CSV_Data/Performance Test/V_8_0_P1-16.csv\n",
      "#### Building vocab ####\n",
      "#### Tokenization ####\n",
      "#### Done ####\n",
      "CPU times: user 4.91 s, sys: 136 ms, total: 5.05 s\n",
      "Wall time: 5.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data=preprocess(test_path,3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c65fed6f-7db9-441f-a8df-92501c36facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.vocab.update(test_data.vocab)\n",
    "# test_data.vocab=train_data.vocab\n",
    "# test_data.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a305e6f7-1018-4238-b3c1-eafdcda98106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# validation_data=preprocess(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85c43581-e246-4ca5-9ffa-d86757878261",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set= classification_set(train_data.x,train_data.y)\n",
    "test_set= classification_set(test_data.x,test_data.y)\n",
    "# validation_set= classification_set(validation_data.x,validation_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e076d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_dataloader = DataLoader(validation_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfea67a-b659-40e1-8282-29248456bfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "487b814b-894b-427c-a715-067944973303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3091435"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b2ee939-d79a-4c8c-8d47-ac7f85b39a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14986772"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20a424-b89c-46a0-9a02-d025b470130b",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd145d-f720-49cb-9dfc-24aae19c7a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "623fe691",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9fae39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import n_gram_cnn\n",
    "# model= n_gram_cnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1912984-eed3-4f7b-bf5f-0e376dad0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cnn\n",
    "model=cnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab5ea927-7bf1-4845-b276-b038d852d745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 3659.3MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.1f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0441daf5-b71b-4895-9c72-f3c84643e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (embeddings): Embedding(7584902, 64)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (conv1): Conv1d(200, 128, kernel_size=(2,), stride=(2,))\n",
       "  (conv2): Conv1d(128, 128, kernel_size=(4,), stride=(2,))\n",
       "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool_2): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d0cce0-c79d-4419-bd79-57dc04e4f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33bb6732-fcbd-4d41-ab0c-30a7cce1f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fdcbe-1720-47f5-9108-9558fb56165c",
   "metadata": {},
   "source": [
    "### Training / Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be257ab1-2e28-420f-b811-1a0a6f4e83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, test_set):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    batch_size = 100\n",
    "    train_dataloader = DataLoader(train_set, batch_size)\n",
    "    test_dataloader = DataLoader(test_set, batch_size)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        predictions1 =[]\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            y_batch = y_batch.type(torch.FloatTensor)\n",
    "            x_batch= x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            # x_batch= x_batch\n",
    "            # y_batch = y_batch\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            #remove squeeze in case batch size > 1\n",
    "            y_batch=torch.squeeze(y_batch)\n",
    "\n",
    "            \n",
    "            loss = F.binary_cross_entropy(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # predictions.append(y_pred.cpu().detach().numpy())\n",
    "\n",
    "            predictions1.extend(list(y_pred.cpu().detach().numpy()))\n",
    "        predictions1=np.round_(predictions1)\n",
    "        predictions = [int(a) for a in predictions1]\n",
    "\n",
    "\n",
    "        test_predictions = evaluation(model, test_dataloader)\n",
    "        train_accuary = calculate_accuray(train_set.y.to_list(), predictions1)\n",
    "        test_accuracy = calculate_accuray(test_set.y.to_list(), test_predictions)\n",
    "        # train_f1_score= calculate_f1_score(train_set.y.to_list(), predictions1)\n",
    "        # test_f1_score= calculate_f1_score(test_set.y.to_list(), test_predictions)\n",
    "        print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f,  Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))\n",
    "        \n",
    "\n",
    "def evaluation(model, test_dataloader):        \n",
    "    # model.eval()\n",
    "    predictions2 = []\n",
    "    # with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dataloader:\n",
    "        \n",
    "#         x_batch= x_batch\n",
    "#         y_batch = y_batch        \n",
    "        x_batch= x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(x_batch)\n",
    "        # y_pred.to(device)\n",
    "        # predictions.append(y_pred.cpu().detach().numpy())\n",
    "        predictions2.extend(list(y_pred.cpu().detach().numpy()))\n",
    "    predictions2=np.round_(predictions2)\n",
    "    predictions=[int(a) for a in predictions2]\n",
    "    return predictions\n",
    "\n",
    "def calculate_accuray(grand_truth, predictions):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    for true, pred in zip(grand_truth, predictions):\n",
    "        if (pred == 1 ) and (true == 1):\n",
    "            true_positives += 1\n",
    "        elif (pred == 0) and (true == 0):\n",
    "            true_negatives += 1\n",
    "        else:\n",
    "            pass\n",
    "    return (true_positives+true_negatives) / len(grand_truth)\n",
    "\n",
    "def calculate_f1_score(grand_truth, predictions):\n",
    "    return f1_score(grand_truth,predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2fda29a-d657-4e75-9f4f-f9ff4494991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.54057, Train accuracy: 0.78813,  Test accuracy: 0.78486\n",
      "Epoch: 2, loss: 0.59890, Train accuracy: 0.78681,  Test accuracy: 0.78133\n",
      "Epoch: 3, loss: 0.62485, Train accuracy: 0.78691,  Test accuracy: 0.78000\n",
      "Epoch: 4, loss: 0.67999, Train accuracy: 0.78820,  Test accuracy: 0.78057\n",
      "Epoch: 5, loss: 0.63386, Train accuracy: 0.78760,  Test accuracy: 0.78333\n",
      "Epoch: 6, loss: 0.54935, Train accuracy: 0.78708,  Test accuracy: 0.78114\n",
      "Epoch: 7, loss: 0.53016, Train accuracy: 0.78832,  Test accuracy: 0.78067\n",
      "Epoch: 8, loss: 0.57875, Train accuracy: 0.78637,  Test accuracy: 0.78467\n",
      "Epoch: 9, loss: 0.58152, Train accuracy: 0.78732,  Test accuracy: 0.77838\n",
      "Epoch: 10, loss: 0.56907, Train accuracy: 0.78892,  Test accuracy: 0.78229\n",
      "CPU times: user 5min 3s, sys: 650 ms, total: 5min 4s\n",
      "Wall time: 5min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model,train_set,test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
