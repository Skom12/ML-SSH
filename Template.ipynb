{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed8a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/myenv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/jovyan/my-conda-envs/myenv/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf679be-78d8-4612-b4c4-3929c6fe8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b8f27",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b18bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir):                                                                                                  \n",
    "    r = []                                                                                                            \n",
    "    subdirs = [x[0] for x in os.walk(dir)]\n",
    "    for subdir in subdirs:\n",
    "        files = os.walk(subdir).__next__()[2]\n",
    "        if (len(files) > 0):                                                                                          \n",
    "            for file in files:\n",
    "                r.append(os.path.join(subdir, file))                   \n",
    "    return r\n",
    "\n",
    "def custom_split(x):\n",
    "    return [x[i:i+20].strip() for i in range(0, len(x), 20)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f92cd",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31410a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='./CSV_Data/Training'\n",
    "test_path='./CSV_Data/Performance Test'\n",
    "validation_path='./CSV_Data/Validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1d8be",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc45dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self,dir_path,start_index,end_index):\n",
    "        self.x=None\n",
    "        self.y = None\n",
    "        self.vocab= dict()\n",
    "        \n",
    "        self.load(dir_path,start_index,end_index)\n",
    "        self.tosplit()\n",
    "        self.build_vocab()\n",
    "        self.tokenize()\n",
    "        print(\"#### Done ####\")\n",
    "        \n",
    "    def len_vocab(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def load(self,dir_path,start_index,end_index):\n",
    "        print('#### Loading files ####')\n",
    "        files=list_files(dir_path)\n",
    "        files.sort()\n",
    "        files=files[start_index:end_index]\n",
    "        dataframes=[]\n",
    "        req_cols=[\"hex_values\",\"class\"]\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            df = pd.read_csv(file,sep='\\t',usecols=req_cols)\n",
    "            dataframes.append(df)\n",
    "        data=pd.concat(dataframes,ignore_index=True)\n",
    "        self.y=data['class']\n",
    "        self.x=data['hex_values']\n",
    "    \n",
    "    def tosplit(self):\n",
    "        for idx, value in self.x.iteritems():\n",
    "            self.x[idx]=custom_split(value)\n",
    "          \n",
    "    def build_vocab(self):  \n",
    "        print('#### Building vocab ####')     \n",
    "        i=1\n",
    "        for idx, value in self.x.iteritems():\n",
    "            for element in value:\n",
    "                if element in self.vocab:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.vocab[element]=i\n",
    "                    i=i+1\n",
    "            \n",
    "    def tokenize(self):\n",
    "        print('#### Tokenization ####')\n",
    "        for idx, value in self.x.iteritems():\n",
    "            for i in range(len(value)):\n",
    "                try:\n",
    "                    value[i]=self.vocab[value[i]]\n",
    "                except:\n",
    "                    value[i]=0\n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63d455",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6dacb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_set(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.to_tensor()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    def __setitem__(self,idx,value):\n",
    "        self.x[idx]=value[0]\n",
    "        self.y[idx]=value[1]\n",
    "    def to_tensor(self):\n",
    "        for i in range(len(self.x)):\n",
    "            temp=list(self[i])\n",
    "            temp[0]=torch.Tensor(temp[0]).int()\n",
    "            temp[1]=float(temp[1])\n",
    "            temp=tuple(temp)\n",
    "            self[i]=temp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876b67ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading files ####\n",
      "./CSV_Data/Training/basic-V_7_8_P1-16.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-24.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-32.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-64.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-16.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-24.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-32.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-64.csv\n",
      "#### Building vocab ####\n",
      "#### Tokenization ####\n",
      "#### Done ####\n",
      "CPU times: user 31.8 s, sys: 2.21 s, total: 34.1 s\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data=preprocess(train_path,21,29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d945dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading files ####\n",
      "./CSV_Data/Performance Test/V_7_1_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_1_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_1_P1-32.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-32.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-32.csv\n",
      "#### Building vocab ####\n",
      "#### Tokenization ####\n",
      "#### Done ####\n",
      "CPU times: user 5.21 s, sys: 125 ms, total: 5.34 s\n",
      "Wall time: 7.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data=preprocess(test_path,0,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c65fed6f-7db9-441f-a8df-92501c36facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.vocab.update(test_data.vocab)\n",
    "# test_data.vocab=train_data.vocab\n",
    "# test_data.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a305e6f7-1018-4238-b3c1-eafdcda98106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# validation_data=preprocess(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c43581-e246-4ca5-9ffa-d86757878261",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set= classification_set(train_data.x,train_data.y)\n",
    "test_set= classification_set(test_data.x,test_data.y)\n",
    "# validation_set= classification_set(validation_data.x,validation_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e076d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_dataloader = DataLoader(validation_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfea67a-b659-40e1-8282-29248456bfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "487b814b-894b-427c-a715-067944973303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2929679"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b2ee939-d79a-4c8c-8d47-ac7f85b39a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14986772"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20a424-b89c-46a0-9a02-d025b470130b",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ae3a85-56ce-4c79-917d-1fd1b3867bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.ModuleList):\n",
    "    def __init__(self):\n",
    "        super(classifier, self).__init__()\n",
    "        self.num_words = 14986772\n",
    "        self.embedding_size = 128\n",
    "        self.seq_len = 200\n",
    "        self.hidden_dim=64\n",
    "        \n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        self.embeddings=nn.Embedding(self.num_words+1, self.embedding_size)\n",
    "        self.flatten=nn.Flatten()\n",
    "        \n",
    "        self.kernel_1=4\n",
    "        self.kernel_2=6\n",
    "        \n",
    "        self.stride = 2\n",
    "        \n",
    "        self.conv1 =nn.Conv1d(self.seq_len, self.hidden_dim, self.kernel_1, self.stride)\n",
    "        self.conv2 =nn.Conv1d(self.hidden_dim, self.hidden_dim, self.kernel_2, self.stride)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, 1)\n",
    "        \n",
    "        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "\n",
    "        x=self.embeddings(x)\n",
    "\n",
    "        out=self.conv1(x)\n",
    "        out=torch.relu(out)\n",
    "        out=self.pool_1(out)\n",
    "        out=self.dropout(out)\n",
    "\n",
    "        out=self.conv2(out)\n",
    "        out=torch.relu(out)\n",
    "        out=self.pool_2(out)\n",
    "        out=self.dropout(out)\n",
    "        out=self.flatten(out)\n",
    "\n",
    "        out=self.fc1(out)\n",
    "        out=torch.relu(out)\n",
    "        out=self.fc2(out)\n",
    "        out=torch.sigmoid(out)\n",
    "\n",
    "        return out.squeeze()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fe691",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9fae39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import n_gram_cnn\n",
    "# model= n_gram_cnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1912984-eed3-4f7b-bf5f-0e376dad0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51d0cce0-c79d-4419-bd79-57dc04e4f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33bb6732-fcbd-4d41-ab0c-30a7cce1f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f091e48b-c3be-4ed0-8c0e-1ef8e86c785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated 7673850368\n",
      "cached 7675576320\n"
     ]
    }
   ],
   "source": [
    "print(\"allocated\",torch.cuda.memory_allocated())\n",
    "print(\"cached\",torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0441daf5-b71b-4895-9c72-f3c84643e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (embeddings): Embedding(14986773, 128)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (conv1): Conv1d(200, 64, kernel_size=(4,), stride=(2,))\n",
       "  (conv2): Conv1d(64, 64, kernel_size=(6,), stride=(2,))\n",
       "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (pool_1): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool_2): MaxPool1d(kernel_size=6, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fdcbe-1720-47f5-9108-9558fb56165c",
   "metadata": {},
   "source": [
    "### Training / Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be257ab1-2e28-420f-b811-1a0a6f4e83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, test_set):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    batch_size = 100\n",
    "    train_dataloader = DataLoader(train_set, batch_size)\n",
    "    test_dataloader = DataLoader(test_set, batch_size)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        predictions1 =[]\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            y_batch = y_batch.type(torch.FloatTensor)\n",
    "            x_batch= x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            # x_batch= x_batch\n",
    "            # y_batch = y_batch\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            #remove squeeze in case batch size > 1\n",
    "            y_batch=torch.squeeze(y_batch)\n",
    "\n",
    "            \n",
    "            loss = F.binary_cross_entropy(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # predictions.append(y_pred.cpu().detach().numpy())\n",
    "\n",
    "            predictions1.extend(list(y_pred.cpu().detach().numpy()))\n",
    "        predictions1=np.round_(predictions1)\n",
    "        predictions = [int(a) for a in predictions1]\n",
    "\n",
    "\n",
    "        test_predictions = evaluation(model, test_dataloader)\n",
    "        train_accuary = calculate_accuray(train_set.y.to_list(), predictions1)\n",
    "        test_accuracy = calculate_accuray(test_set.y.to_list(), test_predictions)\n",
    "        # train_f1_score= calculate_f1_score(train_set.y.to_list(), predictions1)\n",
    "        # test_f1_score= calculate_f1_score(test_set.y.to_list(), test_predictions)\n",
    "        print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f,  Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))\n",
    "        \n",
    "\n",
    "def evaluation(model, test_dataloader):        \n",
    "    model.eval()\n",
    "    predictions2 = []\n",
    "    # with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dataloader:\n",
    "        \n",
    "#         x_batch= x_batch\n",
    "#         y_batch = y_batch        \n",
    "        x_batch= x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(x_batch)\n",
    "        # predictions.append(y_pred.cpu().detach().numpy())\n",
    "        predictions2.extend(list(y_pred.cpu().detach().numpy()))\n",
    "    predictions2=np.round_(predictions2)\n",
    "    predictions=[int(a) for a in predictions2]\n",
    "    return predictions\n",
    "\n",
    "def calculate_accuray(grand_truth, predictions):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    for true, pred in zip(grand_truth, predictions):\n",
    "        if (pred == 1 ) and (true == 1):\n",
    "            true_positives += 1\n",
    "        elif (pred == 0) and (true == 0):\n",
    "            true_negatives += 1\n",
    "        else:\n",
    "            pass\n",
    "    return (true_positives+true_negatives) / len(grand_truth)\n",
    "\n",
    "def calculate_f1_score(grand_truth, predictions):\n",
    "    return f1_score(grand_truth,predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2fda29a-d657-4e75-9f4f-f9ff4494991f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 7.15 GiB (GPU 0; 10.76 GiB total capacity; 7.16 GiB already allocated; 2.53 GiB free; 7.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_set, test_set)\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(y_pred, y_batch)\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# predictions.append(y_pred.cpu().detach().numpy())\u001b[39;00m\n\u001b[1;32m     27\u001b[0m predictions1\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "File \u001b[0;32m~/my-conda-envs/myenv/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/myenv/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 7.15 GiB (GPU 0; 10.76 GiB total capacity; 7.16 GiB already allocated; 2.53 GiB free; 7.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model,train_set,test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
