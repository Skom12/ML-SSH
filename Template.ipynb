{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed8a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/myenv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/jovyan/my-conda-envs/myenv/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf679be-78d8-4612-b4c4-3929c6fe8e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b8f27",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b18bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir):                                                                                                  \n",
    "    r = []                                                                                                            \n",
    "    subdirs = [x[0] for x in os.walk(dir)]\n",
    "    for subdir in subdirs:\n",
    "        files = os.walk(subdir).__next__()[2]\n",
    "        if (len(files) > 0):                                                                                          \n",
    "            for file in files:\n",
    "                r.append(os.path.join(subdir, file))                   \n",
    "    return r\n",
    "\n",
    "def custom_split(x):\n",
    "    return [x[i:i+20].strip() for i in range(0, len(x), 20)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f92cd",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31410a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='./CSV_Data/Training'\n",
    "test_path='./CSV_Data/Performance Test'\n",
    "validation_path='./CSV_Data/Validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1d8be",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc45dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self,dir_path,start_index,end_index):\n",
    "        self.x=None\n",
    "        self.y = None\n",
    "        self.vocab= dict()\n",
    "        \n",
    "        self.load(dir_path,start_index,end_index)\n",
    "        self.tosplit()\n",
    "        self.build_vocab()\n",
    "        self.tokenize()\n",
    "        print(\"#### Done ####\")\n",
    "        \n",
    "    def len_vocab(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def load(self,dir_path,start_index,end_index):\n",
    "        print('#### Loading files ####')\n",
    "        files=list_files(dir_path)\n",
    "        files.sort()\n",
    "        files=files[start_index:end_index]\n",
    "        dataframes=[]\n",
    "        req_cols=[\"hex_values\",\"class\"]\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            df = pd.read_csv(file,sep='\\t',usecols=req_cols)\n",
    "            dataframes.append(df)\n",
    "        data=pd.concat(dataframes,ignore_index=True)\n",
    "        self.y=data['class']\n",
    "        self.x=data['hex_values']\n",
    "    \n",
    "    def tosplit(self):\n",
    "        for idx, value in self.x.iteritems():\n",
    "            self.x[idx]=custom_split(value)\n",
    "          \n",
    "    def build_vocab(self):  \n",
    "        print('#### Building vocab ####')     \n",
    "        i=1\n",
    "        for idx, value in self.x.iteritems():\n",
    "            for element in value:\n",
    "                if element in self.vocab:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.vocab[element]=i\n",
    "                    i=i+1\n",
    "            \n",
    "    def tokenize(self):\n",
    "        print('#### Tokenization ####')\n",
    "        for idx, value in self.x.iteritems():\n",
    "            for i in range(len(value)):\n",
    "                try:\n",
    "                    value[i]=self.vocab[value[i]]\n",
    "                except:\n",
    "                    value[i]=0\n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63d455",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6dacb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_set(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.to_tensor()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    def __setitem__(self,idx,value):\n",
    "        self.x[idx]=value[0]\n",
    "        self.y[idx]=value[1]\n",
    "    def to_tensor(self):\n",
    "        for i in range(len(self.x)):\n",
    "            temp=list(self[i])\n",
    "            temp[0]=torch.Tensor(temp[0]).int()\n",
    "            temp[1]=float(temp[1])\n",
    "            temp=tuple(temp)\n",
    "            self[i]=temp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876b67ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading files ####\n",
      "./CSV_Data/Training/basic-V_7_8_P1-16.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-24.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-32.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-64.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-16.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-24.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-32.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-64.csv\n",
      "#### Building vocab ####\n",
      "#### Tokenization ####\n",
      "#### Done ####\n",
      "CPU times: user 31.1 s, sys: 2.24 s, total: 33.4 s\n",
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data=preprocess(train_path,21,29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d945dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading files ####\n",
      "./CSV_Data/Performance Test/V_7_1_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_1_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_1_P1-32.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-32.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-32.csv\n",
      "#### Building vocab ####\n",
      "#### Tokenization ####\n",
      "#### Done ####\n",
      "CPU times: user 5 s, sys: 79.8 ms, total: 5.08 s\n",
      "Wall time: 5.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data=preprocess(test_path,0,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c65fed6f-7db9-441f-a8df-92501c36facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.vocab.update(test_data.vocab)\n",
    "# test_data.vocab=train_data.vocab\n",
    "# test_data.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a305e6f7-1018-4238-b3c1-eafdcda98106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# validation_data=preprocess(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85c43581-e246-4ca5-9ffa-d86757878261",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set= classification_set(train_data.x,train_data.y)\n",
    "test_set= classification_set(test_data.x,test_data.y)\n",
    "# validation_set= classification_set(validation_data.x,validation_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e076d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_dataloader = DataLoader(validation_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfea67a-b659-40e1-8282-29248456bfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "487b814b-894b-427c-a715-067944973303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2929679"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b2ee939-d79a-4c8c-8d47-ac7f85b39a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14986772"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fe691",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9fae39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.ModuleList):\n",
    "    def __init__(self):\n",
    "        super(classifier, self).__init__()\n",
    "        self.seq_len = 200\n",
    "        self.num_words = 14986772\n",
    "\n",
    "        \n",
    "        self.embedding_size = 32\n",
    "        self.out_size = 256\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.kernel_1=2\n",
    "        self.kernel_2=4\n",
    "        self.kernel_3=8\n",
    "        self.kernel_4 =10 \n",
    "        \n",
    "        self.stride = 2\n",
    "          \n",
    "        self.embedding = nn.Embedding(self.num_words+1, self.embedding_size)\n",
    "        \n",
    "        self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
    "        self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "        self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "        self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "\n",
    "\n",
    "        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "\n",
    "        self.fc = nn.Linear(self.final_len(), 1)\n",
    "\n",
    "    def final_len(self):\n",
    "        out_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_1 = math.floor(out_conv_1)\n",
    "        out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_1 = math.floor(out_pool_1)\n",
    "      \n",
    "        out_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_2 = math.floor(out_conv_2)\n",
    "        out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_2 = math.floor(out_pool_2)\n",
    "      \n",
    "        out_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_3 = math.floor(out_conv_3)\n",
    "        out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_3 = math.floor(out_pool_3)\n",
    "\n",
    "        out_conv_4 = ((self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_4 = math.floor(out_conv_4)\n",
    "        out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_4 = math.floor(out_pool_4)\n",
    "        \n",
    "        return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4)  * self.out_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x1 = self.conv_1(x)\n",
    "        x1 = torch.relu(x1)\n",
    "        x1 = self.pool_1(x1)\n",
    "\n",
    "        \n",
    "        x2 = self.conv_2(x)\n",
    "        x2 = torch.relu((x2)) \n",
    "        x2 = self.pool_2(x2)\n",
    "        \n",
    "        x3 = self.conv_3(x)\n",
    "        x3 = torch.relu(x3)\n",
    "        x3 = self.pool_3(x3)\n",
    "\n",
    "        x4 = self.conv_4(x)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.pool_4(x4)\n",
    "        \n",
    "        union = torch.cat((x1, x2, x3, x4), 2)\n",
    "        union= union.reshape(union.size(0), -1)\n",
    "        out = self.fc(union)\n",
    "        out = self.dropout(out)\n",
    "        out = torch.sigmoid(out)\n",
    "      \n",
    "        return out.squeeze()\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fdcbe-1720-47f5-9108-9558fb56165c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be257ab1-2e28-420f-b811-1a0a6f4e83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, train_set, test_set):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    batch_size = 100\n",
    "    train_dataloader = DataLoader(train_set, batch_size)\n",
    "    test_dataloader = DataLoader(test_set, batch_size)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        predictions1 =[]\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "\n",
    "            y_batch = y_batch.type(torch.FloatTensor)\n",
    "            x_batch= x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            #remove squeeze in case batch size > 1\n",
    "            y_batch=torch.squeeze(y_batch)\n",
    "\n",
    "            \n",
    "            loss = F.binary_cross_entropy(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # predictions.append(y_pred.cpu().detach().numpy())\n",
    "\n",
    "            predictions1.extend(list(y_pred.cpu().detach().numpy()))\n",
    "        predictions1=np.round_(predictions1)\n",
    "        predictions = [int(a) for a in predictions1]\n",
    "\n",
    "\n",
    "        test_predictions = evaluation(model, test_dataloader)\n",
    "        train_accuary = calculate_accuray(train_set.y.to_list(), predictions1)\n",
    "        test_accuracy = calculate_accuray(test_set.y.to_list(), test_predictions)\n",
    "        # train_f1_score= calculate_f1_score(train_set.y.to_list(), predictions1)\n",
    "        # test_f1_score= calculate_f1_score(test_set.y.to_list(), test_predictions)\n",
    "        print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f,  Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13459cf4-3970-4579-8ef5-22089c973b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation(model, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions2 = []\n",
    "    # with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dataloader:\n",
    "        x_batch= x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(x_batch)\n",
    "        # predictions.append(y_pred.cpu().detach().numpy())\n",
    "        predictions2.extend(list(y_pred.cpu().detach().numpy()))\n",
    "    predictions2=np.round_(predictions2)\n",
    "    predictions=[int(a) for a in predictions2]\n",
    "    return predictions\n",
    "\n",
    "def calculate_accuray(grand_truth, predictions):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    for true, pred in zip(grand_truth, predictions):\n",
    "        if (pred == 1 ) and (true == 1):\n",
    "            true_positives += 1\n",
    "        elif (pred == 0) and (true == 0):\n",
    "            true_negatives += 1\n",
    "        else:\n",
    "            pass\n",
    "    return (true_positives+true_negatives) / len(grand_truth)\n",
    "\n",
    "def calculate_f1_score(grand_truth, predictions):\n",
    "    return f1_score(grand_truth,predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d0cce0-c79d-4419-bd79-57dc04e4f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33bb6732-fcbd-4d41-ab0c-30a7cce1f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f091e48b-c3be-4ed0-8c0e-1ef8e86c785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated 0\n",
      "cached 0\n"
     ]
    }
   ],
   "source": [
    "print(\"allocated\",torch.cuda.memory_allocated())\n",
    "print(\"cached\",torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a48eea2-4137-46f0-b327-2d68d7c6d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a957c44-f885-4e89-bf39-a4395d7a5d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7911621369421482\n"
     ]
    }
   ],
   "source": [
    "mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])\n",
    "mem = mem_params + mem_bufs # in bytes\n",
    "mem=mem/1024\n",
    "mem=mem/1024\n",
    "mem=mem/1024\n",
    "print(mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0441daf5-b71b-4895-9c72-f3c84643e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (embedding): Embedding(14986773, 32)\n",
       "  (conv_1): Conv1d(200, 256, kernel_size=(2,), stride=(2,))\n",
       "  (conv_2): Conv1d(200, 256, kernel_size=(4,), stride=(2,))\n",
       "  (conv_3): Conv1d(200, 256, kernel_size=(8,), stride=(2,))\n",
       "  (conv_4): Conv1d(200, 256, kernel_size=(10,), stride=(2,))\n",
       "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool_2): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool_3): MaxPool1d(kernel_size=8, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool_4): MaxPool1d(kernel_size=10, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=4864, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2fda29a-d657-4e75-9f4f-f9ff4494991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.70495, Train accuracy: 0.65623,  Test accuracy: 0.43796\n",
      "Epoch: 2, loss: 0.71717, Train accuracy: 0.65652,  Test accuracy: 0.43796\n",
      "Epoch: 3, loss: 0.78768, Train accuracy: 0.65599,  Test accuracy: 0.43796\n",
      "Epoch: 4, loss: 0.75863, Train accuracy: 0.65705,  Test accuracy: 0.43796\n",
      "Epoch: 5, loss: 0.75729, Train accuracy: 0.65647,  Test accuracy: 0.43796\n",
      "Epoch: 6, loss: 0.71620, Train accuracy: 0.65713,  Test accuracy: 0.43796\n",
      "Epoch: 7, loss: 0.73988, Train accuracy: 0.65710,  Test accuracy: 0.43796\n",
      "Epoch: 8, loss: 0.73501, Train accuracy: 0.65746,  Test accuracy: 0.43796\n",
      "Epoch: 9, loss: 0.72632, Train accuracy: 0.65603,  Test accuracy: 0.43796\n",
      "Epoch: 10, loss: 0.75637, Train accuracy: 0.65649,  Test accuracy: 0.43796\n",
      "CPU times: user 11min 14s, sys: 1.03 s, total: 11min 15s\n",
      "Wall time: 11min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model,train_set,test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
