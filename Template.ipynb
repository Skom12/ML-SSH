{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed8a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/myenv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/jovyan/my-conda-envs/myenv/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf679be-78d8-4612-b4c4-3929c6fe8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b8f27",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b18bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir):                                                                                                  \n",
    "    r = []                                                                                                            \n",
    "    subdirs = [x[0] for x in os.walk(dir)]\n",
    "    for subdir in subdirs:\n",
    "        files = os.walk(subdir).__next__()[2]\n",
    "        if (len(files) > 0):                                                                                          \n",
    "            for file in files:\n",
    "                r.append(os.path.join(subdir, file))                   \n",
    "    return r\n",
    "\n",
    "def custom_split(x):\n",
    "    return [x[i:i+20].strip() for i in range(0, len(x), 20)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f92cd",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31410a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='./CSV_Data/Training'\n",
    "test_path='./CSV_Data/Performance Test'\n",
    "validation_path='./CSV_Data/Validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1d8be",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfc45dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self,dir_path,start_index,end_index):\n",
    "        self.x=None\n",
    "        self.y = None\n",
    "        self.vocab= dict()\n",
    "        \n",
    "        self.load(dir_path,start_index,end_index)\n",
    "        self.tosplit()\n",
    "        self.build_vocab()\n",
    "        self.tokenize()\n",
    "        print(\"#### Done ####\")\n",
    "        \n",
    "    def len_vocab(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def load(self,dir_path,start_index,end_index):\n",
    "        print('#### Loading files ####')\n",
    "        files=list_files(dir_path)\n",
    "        files.sort()\n",
    "        files=files[start_index:end_index]\n",
    "        dataframes=[]\n",
    "        req_cols=[\"hex_values\",\"class\"]\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            df = pd.read_csv(file,sep='\\t',usecols=req_cols)\n",
    "            dataframes.append(df)\n",
    "        data=pd.concat(dataframes,ignore_index=True)\n",
    "        self.y=data['class']\n",
    "        self.x=data['hex_values']\n",
    "    \n",
    "    def tosplit(self):\n",
    "        for idx, value in self.x.iteritems():\n",
    "            self.x[idx]=custom_split(value)\n",
    "          \n",
    "    def build_vocab(self):  \n",
    "        print('#### Building vocab ####')     \n",
    "        i=1\n",
    "        for idx, value in self.x.iteritems():\n",
    "            for element in value:\n",
    "                if element in self.vocab:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.vocab[element]=i\n",
    "                    i=i+1\n",
    "            \n",
    "    def tokenize(self):\n",
    "        print('#### Tokenization ####')\n",
    "        for idx, value in self.x.iteritems():\n",
    "            for i in range(len(value)):\n",
    "                try:\n",
    "                    value[i]=self.vocab[value[i]]\n",
    "                except:\n",
    "                    value[i]=0\n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63d455",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6dacb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_set(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.to_tensor()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    def __setitem__(self,idx,value):\n",
    "        self.x[idx]=value[0]\n",
    "        self.y[idx]=value[1]\n",
    "    def to_tensor(self):\n",
    "        for i in range(len(self.x)):\n",
    "            temp=list(self[i])\n",
    "            temp[0]=torch.Tensor(temp[0]).int()\n",
    "            temp[1]=float(temp[1])\n",
    "            temp=tuple(temp)\n",
    "            self[i]=temp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "876b67ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading files ####\n",
      "./CSV_Data/Training/basic-V_7_8_P1-16.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-24.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-32.csv\n",
      "./CSV_Data/Training/basic-V_7_8_P1-64.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-16.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-24.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-32.csv\n",
      "./CSV_Data/Training/basic-V_7_9_P1-64.csv\n",
      "#### Building vocab ####\n",
      "#### Tokenization ####\n",
      "#### Done ####\n",
      "CPU times: user 34.2 s, sys: 2.3 s, total: 36.5 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data=preprocess(train_path,21,29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d945dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Loading files ####\n",
      "./CSV_Data/Performance Test/V_7_1_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_1_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_1_P1-32.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_8_P1-32.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-16.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-24.csv\n",
      "./CSV_Data/Performance Test/V_7_9_P1-32.csv\n",
      "#### Building vocab ####\n",
      "#### Tokenization ####\n",
      "#### Done ####\n",
      "CPU times: user 5.28 s, sys: 110 ms, total: 5.39 s\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data=preprocess(test_path,0,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c65fed6f-7db9-441f-a8df-92501c36facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.vocab.update(test_data.vocab)\n",
    "# test_data.vocab=train_data.vocab\n",
    "# test_data.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a305e6f7-1018-4238-b3c1-eafdcda98106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# validation_data=preprocess(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85c43581-e246-4ca5-9ffa-d86757878261",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set= classification_set(train_data.x,train_data.y)\n",
    "test_set= classification_set(test_data.x,test_data.y)\n",
    "# validation_set= classification_set(validation_data.x,validation_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e076d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_dataloader = DataLoader(validation_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfea67a-b659-40e1-8282-29248456bfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "487b814b-894b-427c-a715-067944973303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2929679"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b2ee939-d79a-4c8c-8d47-ac7f85b39a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14986772"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fe691",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9fae39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import n_gram_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1912984-eed3-4f7b-bf5f-0e376dad0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= n_gram_cnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d0cce0-c79d-4419-bd79-57dc04e4f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33bb6732-fcbd-4d41-ab0c-30a7cce1f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f091e48b-c3be-4ed0-8c0e-1ef8e86c785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated 1923833344\n",
      "cached 1941962752\n"
     ]
    }
   ],
   "source": [
    "print(\"allocated\",torch.cuda.memory_allocated())\n",
    "print(\"cached\",torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0441daf5-b71b-4895-9c72-f3c84643e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (embedding): Embedding(14986773, 32)\n",
       "  (conv_1): Conv1d(200, 256, kernel_size=(2,), stride=(2,))\n",
       "  (conv_2): Conv1d(200, 256, kernel_size=(4,), stride=(2,))\n",
       "  (conv_3): Conv1d(200, 256, kernel_size=(8,), stride=(2,))\n",
       "  (conv_4): Conv1d(200, 256, kernel_size=(10,), stride=(2,))\n",
       "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool_2): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool_3): MaxPool1d(kernel_size=8, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool_4): MaxPool1d(kernel_size=10, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=4864, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fdcbe-1720-47f5-9108-9558fb56165c",
   "metadata": {},
   "source": [
    "### Training / Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be257ab1-2e28-420f-b811-1a0a6f4e83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, test_set):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    batch_size = 100\n",
    "    train_dataloader = DataLoader(train_set, batch_size)\n",
    "    test_dataloader = DataLoader(test_set, batch_size)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        predictions1 =[]\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "\n",
    "            y_batch = y_batch.type(torch.FloatTensor)\n",
    "            x_batch= x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            #remove squeeze in case batch size > 1\n",
    "            y_batch=torch.squeeze(y_batch)\n",
    "\n",
    "            \n",
    "            loss = F.binary_cross_entropy(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # predictions.append(y_pred.cpu().detach().numpy())\n",
    "\n",
    "            predictions1.extend(list(y_pred.cpu().detach().numpy()))\n",
    "        predictions1=np.round_(predictions1)\n",
    "        predictions = [int(a) for a in predictions1]\n",
    "\n",
    "\n",
    "        test_predictions = evaluation(model, test_dataloader)\n",
    "        train_accuary = calculate_accuray(train_set.y.to_list(), predictions1)\n",
    "        test_accuracy = calculate_accuray(test_set.y.to_list(), test_predictions)\n",
    "        # train_f1_score= calculate_f1_score(train_set.y.to_list(), predictions1)\n",
    "        # test_f1_score= calculate_f1_score(test_set.y.to_list(), test_predictions)\n",
    "        print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f,  Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))\n",
    "        \n",
    "\n",
    "def evaluation(model, test_dataloader):        \n",
    "    model.eval()\n",
    "    predictions2 = []\n",
    "    # with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dataloader:\n",
    "        \n",
    "        x_batch= x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(x_batch)\n",
    "        # predictions.append(y_pred.cpu().detach().numpy())\n",
    "        predictions2.extend(list(y_pred.cpu().detach().numpy()))\n",
    "    predictions2=np.round_(predictions2)\n",
    "    predictions=[int(a) for a in predictions2]\n",
    "    return predictions\n",
    "\n",
    "def calculate_accuray(grand_truth, predictions):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    for true, pred in zip(grand_truth, predictions):\n",
    "        if (pred == 1 ) and (true == 1):\n",
    "            true_positives += 1\n",
    "        elif (pred == 0) and (true == 0):\n",
    "            true_negatives += 1\n",
    "        else:\n",
    "            pass\n",
    "    return (true_positives+true_negatives) / len(grand_truth)\n",
    "\n",
    "def calculate_f1_score(grand_truth, predictions):\n",
    "    return f1_score(grand_truth,predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2fda29a-d657-4e75-9f4f-f9ff4494991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.70495, Train accuracy: 0.65623,  Test accuracy: 0.43796\n",
      "Epoch: 2, loss: 0.71717, Train accuracy: 0.65652,  Test accuracy: 0.43796\n",
      "Epoch: 3, loss: 0.78768, Train accuracy: 0.65599,  Test accuracy: 0.43796\n",
      "Epoch: 4, loss: 0.75863, Train accuracy: 0.65705,  Test accuracy: 0.43796\n",
      "Epoch: 5, loss: 0.75729, Train accuracy: 0.65647,  Test accuracy: 0.43796\n",
      "Epoch: 6, loss: 0.71620, Train accuracy: 0.65713,  Test accuracy: 0.43796\n",
      "Epoch: 7, loss: 0.73988, Train accuracy: 0.65710,  Test accuracy: 0.43796\n",
      "Epoch: 8, loss: 0.73501, Train accuracy: 0.65746,  Test accuracy: 0.43796\n",
      "Epoch: 9, loss: 0.72632, Train accuracy: 0.65603,  Test accuracy: 0.43796\n",
      "Epoch: 10, loss: 0.75637, Train accuracy: 0.65649,  Test accuracy: 0.43796\n",
      "CPU times: user 11min 14s, sys: 1.03 s, total: 11min 15s\n",
      "Wall time: 11min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model,train_set,test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
